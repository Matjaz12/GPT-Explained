# Building a Generative Pre-trained Transformer (GPT)

![transformer](./img/transformer.jpg)


## About

In this notebook, we aim to bridge the gap between theory and practical implementation by implementing a decoder-only Transformer model for character-level prediction. The majority of the code used in this project has been sourced from [1], while the theoretical insights have been taken from various sources including [2], [3], [4], [5]. Additionally, illustrations used in this notebook are taken from the "Attention is all you need" paper [6].

## References:

[1] https://www.youtube.com/watch?v=kCc8FmEb1nY&t=8s&ab_channel=AndrejKarpathy

[2] https://www.youtube.com/watch?v=yGTUuEx3GkA&ab_channel=Rasa

[3] https://www.youtube.com/watch?v=tIvKXrEDMhk&ab_channel=Rasa

[4] https://www.youtube.com/watch?v=23XUv0T9L5c&ab_channel=Rasa

[5] https://www.youtube.com/watch?v=EXNBy8G43MM&ab_channel=Rasa

[6] https://arxiv.org/abs/1706.03762
